---
permalink: /
title: "Robin Eshraghi"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm Mohammad Amir, and I go by <b>Robin</b> in daily life! 

I'm an Engineer and Researcher with a background in Artificial Intelligence, Robotics, and Electronics. I am currently interested in intelligence autonomous machines that sense, adapt, and connect to advance embodied intelligence, drawing inspiration from natural intelligence in humans and animals.


## Research and Projects (selected)
<ul>

  <!-- Project 1 -->
  <li>
    <strong>[2025-26] 3D Semantic Object Grounding for Vision-Language-Aware Robots</strong>
    <ul>
      <li>Embodied AI Software Engineer (GRA): Working on a DARPA-funded project advancing 3D semantic object grounding for vision-language-aware robots.</li>
      <li>Develop AI modules enabling the Spot robot to detect, label, and localize household objects in 3D space to build a semantic world map for language-guided navigation.</li>
      <li>Design multimodal pipelines, maintain simulation environments, and conduct testing/debugging for milestone-based research progress.</li>
      <li>Keywords: Embodied AI, VLMs, 3D Object Grounding, Multimodal Perception, World Modeling, ROS2, Isaac Sim, Habitat-Sim, Python, Docker, Git, AWS, Linux.</li>

      <li>
        Links (previous API modules and tests):
        <a href="https://github.com/MAmirEshraghi/perception_tiamat_drail">[GitHub]</a>
      </li>

      <li>
        <div class="video-row">

          <!-- Video 1 -->
          <div class="video-item">
            <h4>Demo: Spot robot perception & language-guided navigation in IsaacSim (08/25/2025)</h4>
            <div class="video-container">
              <iframe
                src="https://www.youtube-nocookie.com/embed/DUtXddkQi6I?rel=0&modestbranding=1&iv_load_policy=3&fs=1&cc_load_policy=0&playsinline=1&autohide=1&controls=1&enablejsapi=1&origin=https://robin-eshraghi.github.io"
                title="Spot robot navigation"
                frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen loading="lazy"></iframe>
            </div>
          </div>

          <!-- Video 2 -->
          <div class="video-item">
            <h4>5-Minute Project Update in Habitat (Weekly Report Sample) (10/30/2025)</h4>
            <div class="video-container">
              <iframe
                src="https://www.youtube-nocookie.com/embed/SwY2Jn_4mQ0?rel=0&modestbranding=1&iv_load_policy=3&fs=1&cc_load_policy=0&playsinline=1&autohide=1&controls=1&enablejsapi=1&origin=https://robin-eshraghi.github.io"
                title="weekly update sample"
                frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen loading="lazy"></iframe>
            </div>
          </div>

        </div>
      </li>
    </ul>
  </li>

  <!-- Project 2 -->
  <li>
    <strong>[2025] Path Planning and Trajectory Generation for Robotic Arm in Apple Harvesting</strong>
    <ul>
      <li>Developed a simulation-based framework for planning and executing collision-free trajectories for a UR5 robotic arm for autonomous apple harvesting.</li>
      <li>Built RRT/RRT-Connect modules with collision checking, reachability analysis, and visibility constraints. Integrated the full pipeline in ROS and PyBullet with environment setup, kinematics, and testing routines. Enabled smooth trajectory generation for reproducible experiments in agricultural robotic manipulation.</li>
      <li>Keywords: Path Planning, RRT/RRT-Connect, Trajectory Generation, Collision Checking, UR5 Manipulator, Kinematics, PyBullet, ROS/ROS2, Linux, Python.</li>
      <li>
        Links:
        <a href="https://github.com/OSUrobotics/pybullet-tree-sim/tree/feature/apple_path_planning_robinEsh">[GitHub]</a>
        <a href="https://www.youtube.com/watch?v=L9cdALAOvzs">[Demo]</a>
        <br/>
        <img src="/assets/armRobot1.gif" alt="arm" style="max-width:400px;" />
      </li>
    </ul>
  </li>

  <!-- Project 3 -->
  <li>
    <strong>[2023] AI-based Leukemia Diagnosis Mobile Application</strong>
    <ul>
      <li>Developed a smartphone-based AI system for detection and staging of leukemia from microscopic blood cell images.</li>
      <li>
        Links:
        <a href="https://doi.org/10.1016/j.imu.2023.101244">[Paper]</a>
        <a href="https://github.com/MAmirEshraghi/Lightweight-Deep-CNN-Based-Mobile-App-in-the-Screening-of-ALL">[GitHub]</a>
        <a href="https://www.kaggle.com/datasets/mohammadamireshraghi/blood-cell-cancer-all-4class">[Dataset]</a>
        <br/>
        <img src="/assets/images/LeukemiaClassificatio1.jpg" alt="L1" style="max-width:800px;" />
        <img src="/assets/images/LeukemiaClassificatio2.png" alt="L2" style="max-width:500px;" />
      </li>
    </ul>
  </li>

</ul>

## Publications
<ul>

  <li>
    COV-MobNets: A Mobile Networks Ensemble Model for Diagnosis of COVID-19  
    <br/>
    <font size="3"><b>MA. Eshraghi</b>, A. Ayatollahi, SB. Shokouhi</font>
    <br/>
    <font size="2">
      <a href="https://bmcmedimaging.biomedcentral.com/">[BMC Medical Imaging, 2023]</a>
      <a href="https://doi.org/10.1186/s12880-023-01039-w">[doi]</a>
      <a href="https://github.com/MAmirEshraghi/Lightweight-Deep-CNN-Based-Mobile-App-in-the-Screening-of-ALL">[GitHub]</a>
    </font>
  </li>

  <li>
    A Mobile Application based on Efficient Lightweight CNN Model for Classification of B-ALL  
    <br/>
    <font size="3">(A. Hosseini, <b>MA. Eshraghi</b>, T. Taami, H. Sadeghsalehi, Z. Hoseinzadeh, M. Rafiee, M. Ghaderzadeh)</font>
    <br/>
    <font size="2">
      <a href="https://bmcmedimaging.biomedcentral.com/">[Informatics in Medicine Unlocked, 2023]</a>
      <a href="https://doi.org/10.1016/j.imu.2023.101244">[doi]</a>
      <a href="https://github.com/MAmirEshraghi/Lightweight-Deep-CNN-Based-Mobile-App-in-the-Screening-of-ALL">[GitHub]</a>
    </font>
  </li>

  <li>
    Efficient Framework for Detection of COVID-19 Omicron and Delta Variants  
    <br/>
    <font size="3">(M. Ghaderzadeh, <b>MA. Eshraghi</b>, F. Asadi, A. Hosseini, R. Jafari, D. Bashash, H. Abolghasemi)</font>
    <br/>
    <font size="2">
      <a href="https://bmcmedimaging.biomedcentral.com/">[Computational and Mathematical Methods in Medicine, 2022]</a>
      <a href="https://doi.org/10.1155/2022/4838009">[doi]</a>
      <a href="https://github.com/MAmirEshraghi/Deep_Covid19_Detection_Overall_framework">[GitHub]</a>
    </font>
  </li>

</ul>


## Education


<ul>
    <li><b>M.Sc. in Artificial Intelligence</b>, Oregon State University (OSU), OR, USA
      <br />
    <font size="3">
    Graduate Research Assistant
    </font> 
    <br /> 
    </li>
</ul>

<ul>
    <li><b>M.Sc. in Digital Electronic Systems</b>, University of Science and Technology (IUST)
      <br />
    <font size="3">
    First-rank GPA Scholarship
    </font> 
    <br /> 
    </li>
</ul>

<ul>
    <li><b>B.E | A.E. in Electronic Engineering</b>, Technical and Vocational University (TVU)
    </li>
</ul>
<ul>
    <li><b>Diploma in Electronics </b>, Technical and Vocational School
    </li>
</ul>
## Professional Experiences

<ul>
    <li><b>Graduate Research Assistant</b>,  Oregon State University (OSU), 
    <br />
    <font size="3">
      Dynamic Robotics and Artificial intelligence Laboratory (DRAIL) |
      Spot Robot Perception System
    </font> 
    <br /> 
    </li>
</ul>

<ul>
    <li><b>Project Internship</b>,  Oregon State University (OSU), 
    <br />
    <font size="3">
      Robotics Lab | Apple-Picking Simulation
    </font> 
    <br /> 
    </li>
</ul>

<ul>
    <li><b>Research Assistant</b>,  University of Science and Technology (IUST), 
    <br />
    <font size="3">
      Machine Vision Lab | Developed AI-based Covid-19 Detection Systems, resulting in one publication.
    </font> 
    <br /> 
    </li>
</ul>
<ul>
    <li><b>Research Collaborator</b>, Shahid Beheshti University of Medical Sciences (SBUMS), 
    <br />
    <font size="3">
      Hematology Research Group |
      Developed Leukemia Diagnosis AI-based Mobile Application, resulting in one publication, oral presentation, and a proposal. 
    </font> 
    <br /> 
    </li>
</ul>
<ul>
    <li><b>Research Collaborator</b>, Shahid Beheshti University of Medical Sciences (SBUMS), Iran      
    <br />
    <font size="3">
      Radiology Research Group |
      Developed AI-based Covid-19 Diagnosis Systems, resulting in one publication.
    </font> 
    <br /> 
    </li>
</ul>


## Other Research and Projects:

<ul>
<li>
    <strong>[2024] Weed Detection and Mapping System</strong>
    <ul>      
      <li>Developed an object detection system processing high-resolution drone orthomosaics through a custom computer vision pipeline to detect weeds and generate GIS coordinates to support autonomous sprayer tractors to perform targeted herbicide application 
      <a href="https://github.com/MAmirEshraghi/WeedID_YOLOs">[GitHub]</a>
      <br/>
      <img src="/assets/images/W1.png" alt="w1" style="max-width:300px;" />
      <img src="/assets/images/w2.gif" alt="w2" style="max-width:200px;" />
      </li>
    </ul>
  </li>
  
  <li>
    <strong>[2021-22] AI-based Covid-19 Diagnosis Systems</strong>
    <ul>
      <li> <strong>(1) COV-MobNets framework:</strong> Combined lightweight CNN and ViT models for X-ray classification</li>
      <li>
      Links:
      <a href="https://doi.org/10.1186/s12880-023-01039-w"> [Paper] </a><a href="https://github.com/MAmirEshraghi/COV-MobNets">[GitHub] </a><a href="https://www.kaggle.com/code/mohammadamireshraghi/cov-mobnets-for-diagnosis-covid-19-based-on-x-ray">[Kaggle] </a>
      <br/>
      <img src="/assets/images/COV06.png" alt="cov" style="max-width:250px;" />      
      </li>
      <li> <strong>(2) Dual-phase CNN-based framework:</strong> Modified ResNet and DenseNet models. Utilized image augmentations </li>
      <li>
      Links:  
      <a href="https://doi.org/10.1155/2022/4838009"> [Paper] </a><a href="https://github.com/MAmirEshraghi/Deep_Covid19_Detection_Overall_framework"> [GitHub] </a>  <a href="https://www.kaggle.com/datasets/mohammadamireshraghi/covid19-omicron-and-delta-variant-ct-scan-dataset"> [Data_Kaggle] </a> 
      <br/>
      <img src="/assets/images/O_COV2.png" alt="cov1" style="max-width:250px;" />      
      </li>
    </ul>
  </li> 

  <li>
    <strong>2017-2019</strong>
    <ul>
      <li>
      Advanced Line-Following Robot 
      <a href="https://github.com/MAmirEshraghi/Line_Follower_Robot"> [GitHub] </a>
      <br/>
      <img src="/assets/images/rob1.jpg" alt="R1" style="max-width:250px;" />      
      <img src="/assets/images/R02.jpg" alt="R2" style="max-width:250px;" />
      <img src="/assets/images/R04.gif" alt="R3" style="max-width:100px;" />
      </li>   
    </ul>
  </li>  
  
</ul>



## Gallery 

<img src="/assets/images/m11.jpg" alt="L1" style="max-width:300px;" /> 

<style>
.video-row {
  display: flex;
  flex-wrap: wrap;
  gap: 10px;
  justify-content: flex-start;
  margin: 10px 0 20px 0;
}

.video-item {
  flex: 1 1 calc(33.333% - 10px);
  max-width: calc(33.333% - 10px);
  min-width: 280px;
}

.video-item h4 {
  margin: 0 0 5px 0 !important;
  padding: 0 !important;
  font-size: 0.85em;
  font-weight: 600;
  color: #2c3e50;
  line-height: 1.1;
  text-align: center;
}

.video-container {
  position: relative;
  width: 100%;
  padding-top: 25%; /* Much shorter - wide landscape format */
  background: #000;
  border-radius: 4px;
  overflow: hidden;
  box-shadow: 0 2px 6px rgba(0,0,0,0.08);
}

.video-container iframe {
  position: absolute !important;
  top: 0 !important;
  left: 0 !important;
  width: 100% !important;
  height: 100% !important;
  border: 0 !important;
}

@media (max-width: 900px) {
  .video-item {
    flex: 1 1 calc(50% - 10px);
    max-width: calc(50% - 10px);
  }
}

@media (max-width: 600px) {
  .video-item {
    flex: 1 1 100%;
    max-width: 100%;
  }
  
  .video-container {
    padding-top: 233.33%; /* Standard 16:9 on mobile */
  }
}
</style>
