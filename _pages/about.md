---
permalink: /
title: "Robin Eshraghi"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm Mohammad Amir, and I go by <b>Robin</b> in daily life! 

I'm an Engineer and Researcher with a background in Artificial Intelligence, Robotics, and Electronics. I am currently interested in intelligence autonomous machines that sense, adapt, and connect to advance embodied intelligence, drawing inspiration from natural intelligence in humans and animals.



## Research and Projects
<ul>

<li>
  <strong>[2025-26] 3D Semantic Object Grounding for Vision-Language-Aware Robots </strong> 
    <ul> 
      <li>Embodied AI Software Engineer (GRA): I am working on a DARPA-funded project advancing 3D semantic object grounding for vision-language-aware robots.</li> 
        <li>I develop and integrate AI modules that enable the Spot robot to detect, label, and localize household objects in 3D space, building a semantic world map that supports natural-language queries and intelligent navigation.</li> 
        <li>My role includes designing vision-language-planning pipelines, maintaining simulation environments, and conducting testing and debugging to drive milestone-based research progress.</li> 
        <li>Keywords: Embodied AI, Vision-Language Models (VLMs), 3D Semantic Object Grounding, Multimodal Perception, World Modeling, ROS2, Isaac Sim, Habitat-Sim, Python, Docker, Git, AWS, Linux, systematic problem-solving.</li> 

   
  <li>Links (only privious API modlues and tests):
      <a href="https://github.com/MAmirEshraghi/perception_tiamat_drail">[GitHub]</a> 
      <br/>
  </li>
  
  <li>
  <div class="video-row">

<!-- Video 2 -->
  <div class="video-item">
    <h4>(08/25/2025) Demo: Spot robot perception and language guided navigation in Isaacsim</h4>
    <div class="video-container">
      <iframe
        src="https://www.youtube-nocookie.com/embed/DUtXddkQi6I?rel=0&modestbranding=1&iv_load_policy=3&fs=1&cc_load_policy=0&playsinline=1&autohide=1&controls=1&enablejsapi=1&origin=https://robin-eshraghi.github.io"
        title="Spot robot navigation"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen
        loading="lazy">
      </iframe>
    </div>
  </div>
    
  <!-- Video 1 -->
  <div class="video-item">
    <h4>(10/30/2025) 5-Minute Project Update in Habitat (the weekly report sample)</h4>
    <div class="video-container">
      <iframe
        src="https://www.youtube-nocookie.com/embed/SwY2Jn_4mQ0?rel=0&modestbranding=1&iv_load_policy=3&fs=1&cc_load_policy=0&playsinline=1&autohide=1&controls=1&enablejsapi=1&origin=https://robin-eshraghi.github.io"
        title="weekly update sample"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen
        loading="lazy">
      </iframe>
    </div>
  </div>
</li>
</ul>
</li>

  <li>
    <strong>[2025] Path Planning and Trajectory Generation for Robotic Arm in Apple Harvesting </strong>
    <ul>
    <li>Developed a simulation-based framework for planning and executing collision-free trajectories for a UR5 robotic arm to perform autonomous apple harvesting.</li>
    <li>uilt RRT/RRT-Connect motion-planning modules with collision checking, reachability analysis, and visibility constraints to ensure natural and efficient arm movement. Integrated the full pipeline in ROS and PyBullet, including environment setup, kinematics functions, and testing routines.</li>
    <li>Achieved smooth, reliable trajectory generation in simulation, enabling reproducible experiments for agricultural robotic manipulation and future real-world deployment.</li>
      <li>Kekwords: Path Planning, RRT/RRT-Connect, Trajectory Generation, Collision Checking, UR5 Manipulator, Kinematics, PyBullet, ROS/ROS2, Linux, Python, Systematic Debugging.</li>
        <li>
          Links:
      <a href="https://github.com/OSUrobotics/pybullet-tree-sim/tree/feature/apple_path_planning_robinEsh">[GitHub]</a> <a href="https://www.youtube.com/watch?v=L9cdALAOvzs">[Demo (YouTube)]</a>
      <br/>
      <img src="/assets/armRobot1.gif" alt="arm" style="max-width:400px;" />
      </li>
      </ul>
  </li>
  
  <li>
    <strong>[2024] Weed Detection and Mapping System</strong>
    <ul>      
      <li>Developed an object detection system processing high-resolution drone orthomosaics through a custom computer vision pipeline to detect weeds and generate GIS coordinates to support autonomous sprayer tractors to perform targeted herbicide application 
      <a href="https://github.com/MAmirEshraghi/WeedID_YOLOs">[GitHub]</a>
      <br/>
      <img src="/assets/images/W1.png" alt="w1" style="max-width:300px;" />
      <img src="/assets/images/w2.gif" alt="w2" style="max-width:200px;" />
      </li>
    </ul>
  </li>

<li>
    <strong>[2023] AI-based Leukemia Diagnosis Mobile Application </strong>
    <ul>
      <li>Developed a smartphone-based AI System for detection and staging of leukemia in blood cells using microscopic images.</li>
      <li>Links:  
      <a href="https://doi.org/10.1016/j.imu.2023.101244"> [Paper] </a><a href="https://github.com/MAmirEshraghi/Lightweight-Deep-CNN-Based-Mobile-App-in-the-Screening-of-ALL">[GitHub] </a><a href="https://www.kaggle.com/datasets/mohammadamireshraghi/blood-cell-cancer-all-4class"> [Data_Kaggle]</a>
      <br/>
      <img src="/assets/images/LeukemiaClassificatio1.jpg" alt="L1" style="max-width:300px;" />      
      <img src="/assets/images/LeukemiaClassificatio2.png" alt="L2" style="max-width:300px;" />
      </li>
    </ul>
  </li>
  
  <li>
    <strong>[2021-22] AI-based Covid-19 Diagnosis Systems</strong>
    <ul>
      <li> <strong>(1) COV-MobNets framework:</strong> Combined lightweight CNN and ViT models for X-ray classification</li>
      <li>
      Links:
      <a href="https://doi.org/10.1186/s12880-023-01039-w"> [Paper] </a><a href="https://github.com/MAmirEshraghi/COV-MobNets">[GitHub] </a><a href="https://www.kaggle.com/code/mohammadamireshraghi/cov-mobnets-for-diagnosis-covid-19-based-on-x-ray">[Kaggle] </a>
      <br/>
      <img src="/assets/images/COV06.png" alt="cov" style="max-width:250px;" />      
      </li>
      <li> <strong>(2) Dual-phase CNN-based framework:</strong> Modified ResNet and DenseNet models. Utilized image augmentations </li>
      <li>
      Links:  
      <a href="https://doi.org/10.1155/2022/4838009"> [Paper] </a><a href="https://github.com/MAmirEshraghi/Deep_Covid19_Detection_Overall_framework"> [GitHub] </a>  <a href="https://www.kaggle.com/datasets/mohammadamireshraghi/covid19-omicron-and-delta-variant-ct-scan-dataset"> [Data_Kaggle] </a> 
      <br/>
      <img src="/assets/images/O_COV2.png" alt="cov1" style="max-width:250px;" />      
      </li>
    </ul>
  </li> 

  <li>
    <strong>2017-2019</strong>
    <ul>
      <li>
      Advanced Line-Following Robot 
      <a href="https://github.com/MAmirEshraghi/Line_Follower_Robot"> [GitHub] </a>
      <br/>
      <img src="/assets/images/rob1.jpg" alt="R1" style="max-width:250px;" />      
      <img src="/assets/images/R02.jpg" alt="R2" style="max-width:250px;" />
      <img src="/assets/images/R04.gif" alt="R3" style="max-width:100px;" />
      </li>   
    </ul>
  </li>  
  
</ul>


## Publications
<ul>
    <li>COV-MobNets: A Mobile Networks Ensemble Model for Diagnosis of COVID-19 based on Chest X-ray Images
    <br />
    <font size="3">
      (<b>MA. Eshraghi</b>, A. Ayatollahi, SB. Shokouhi)
    </font> 
    <br /> 
    <font size="2">
      <a href="https://bmcmedimaging.biomedcentral.com/">
      [BMC Medical Imaging, 2023]
      </a>
      <a href="https://doi.org/10.1186/s12880-023-01039-w">
      [doi]
      </a>
      <a href="https://github.com/MAmirEshraghi/Lightweight-Deep-CNN-Based-Mobile-App-in-the-Screening-of-ALL">
      [GitHub]
      </a>
    </font>
    </li>
</ul>
<ul>
    <li>A Mobile Application based on Efficient Lightweight CNN model for Classification of B-ALL Cancer from non-cancerous Cells: A Design and Implementation Study
    <br />
    <font size="3">
      (A. Hosseini, <b>MA. Eshraghi</b>, T. Taami, H. Sadeghsalehi, Z. Hoseinzadeh, M. Rafiee, M. Ghaderzadeh)
    </font> 
    <br /> 
    <font size="2">
      <a href="https://bmcmedimaging.biomedcentral.com/">
      [Informatics in Medicine Unlocked, 2023]
      </a>
      <a href="https://doi.org/10.1016/j.imu.2023.101244">
      [doi]
      </a>
      <a href="https://github.com/MAmirEshraghi/Lightweight-Deep-CNN-Based-Mobile-App-in-the-Screening-of-ALL">
      [GitHub]
      </a>
    </font>
    </li>
</ul>
<ul>
    <li>Efficient Framework for Detection of COVID-19 Omicron and Delta Variants Based on Two Intelligent Phases of CNN Models
    <br />
    <font size="3">
      (M. Ghaderzadeh, <b>MA. Eshraghi</b>, F. Asadi, A. Hosseini, R. Jafari, D. Bashash, H. Abolghasemi)
    </font> 
    <br /> 
    <font size="2">
      <a href="https://bmcmedimaging.biomedcentral.com/">
      [Computational and Mathematical Methods in Medicine, 2022]
      </a>
      <a href="https://doi.org/10.1155/2022/4838009">
      [doi]
      </a>
      <a href="https://github.com/MAmirEshraghi/Deep_Covid19_Detection_Overall_framework">
      [GitHub]
      </a>
    </font>
    </li>
</ul>

## Academic Experiences


<ul>
    <li><b>Graduate Research Assistant</b>,  Oregon State University (OSU), 
    <br />
    <font size="3">
      Dynamic Robotics and Artificial intelligence Laboratory (DRAIL), (Prof. Alan Fern) |
      Spot Robot Perception System: as an AI Software Developer, contribute to a real-time scene understanding for the Spot robot to enable language-guided navigation
    </font> 
    <br /> 
    </li>
</ul>

<ul>
    <li><b>Project Internship</b>,  Oregon State University (OSU), 
    <br />
    <font size="3">
      Robotics Lab (Prof. Cindy Grimm) |
      Apple-Picking Simulation: Contributed to the simulation development of apple trees, implemented RRT path planning with integration of camera check on Ur5 to generate collision-free trajectories to enable apple approaching by training RL agent within behavioral cloning.
    </font> 
    <br /> 
    </li>
</ul>

<ul>
    <li><b>Research Assistant</b>,  University of Science and Technology (IUST), 
    <br />
    <font size="3">
      Machine Vision Lab, (Dr. Baradaran Shahriar Shokouhi) |
      Developed AI-based Covid-19 Detection Systems, resulting in one publication.
    </font> 
    <br /> 
    </li>
</ul>
<ul>
    <li><b>Research Collaborator</b>, Shahid Beheshti University of Medical Sciences (SBUMS), 
    <br />
    <font size="3">
      Hematology Research Group, (Dr. Mustafa Ghaderzadeh) |
      Developed Leukemia Diagnosis AI-based Mobile Application, resulting in one publication, oral presentation, and a proposal. 
    </font> 
    <br /> 
    </li>
</ul>
<ul>
    <li><b>Research Collaborator</b>, Shahid Beheshti University of Medical Sciences (SBUMS), Iran      
    <br />
    <font size="3">
      Radiology Research Group, (Dr. Mustafa Ghaderzadeh) |
      Developed AI-based Covid-19 Diagnosis Systems, resulting in one publication.
    </font> 
    <br /> 
    </li>
</ul>

## Education


<ul>
    <li><b>M.Sc. in Artificial Intelligence</b>, Oregon State University  
      <br />
    <font size="3">
    Graduate Research Assistant
    </font> 
    <br /> 
    </li>
</ul>

<ul>
    <li><b>M.Sc. in Digital Electronic Systems</b>, University of Science and Technology (IUST)
      <br />
    <font size="3">
    First-rank GPA, GPA= 18.78/20
    </font> 
    <br /> 
    </li>
</ul>

<ul>
    <li><b>B.E | A.E. in Electronic Engineering</b>, Technical and Vocational University
      <br />
    <font size="3">
    </font> 
    <br /> 
    </li>
</ul>
<ul>
    <li><b>Diploma in Electronics </b>, Technical and Vocational School
      <br />
    <font size="3">
    </font> 
    <br /> 
    </li>
</ul>

## Gallery 

<img src="/assets/images/m11.jpg" alt="L1" style="max-width:300px;" /> 

<style>
.video-row {
  display: flex;
  flex-wrap: wrap;
  gap: 10px;
  justify-content: flex-start;
  margin: 10px 0 20px 0;
}

.video-item {
  flex: 1 1 calc(33.333% - 10px);
  max-width: calc(33.333% - 10px);
  min-width: 280px;
}

.video-item h4 {
  margin: 0 0 5px 0 !important;
  padding: 0 !important;
  font-size: 0.85em;
  font-weight: 600;
  color: #2c3e50;
  line-height: 1.1;
  text-align: center;
}

.video-container {
  position: relative;
  width: 100%;
  padding-top: 25%; /* Much shorter - wide landscape format */
  background: #000;
  border-radius: 4px;
  overflow: hidden;
  box-shadow: 0 2px 6px rgba(0,0,0,0.08);
}

.video-container iframe {
  position: absolute !important;
  top: 0 !important;
  left: 0 !important;
  width: 100% !important;
  height: 100% !important;
  border: 0 !important;
}

@media (max-width: 900px) {
  .video-item {
    flex: 1 1 calc(50% - 10px);
    max-width: calc(50% - 10px);
  }
}

@media (max-width: 600px) {
  .video-item {
    flex: 1 1 100%;
    max-width: 100%;
  }
  
  .video-container {
    padding-top: 233.33%; /* Standard 16:9 on mobile */
  }
}
</style>
